{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woHnuhYHCNn9"
   },
   "source": [
    "# DataScience: Convolutional Neural Networks\r\n",
    "\r\n",
    "**Your name: Tyler Lott** \r\n",
    "\r\n",
    "\r\n",
    "In this homework, we will implement and train a CNN model on CIFAR-10.\r\n",
    "\r\n",
    "**The rule of this homework is to design an advanced CNN model to achieve good performance on CIFAR-10.** The end of this notebooks has several hints to improve your results. \r\n",
    "\r\n",
    "\r\n",
    "**Please list your modifications below**:\r\n",
    "<br/>\r\n",
    "Did everything in Tensorflow because I am more familiar with it\r\n",
    "- Loaded data directly from downloaded files instead of torch dataloader\r\n",
    "- Implemented a ResNet CNN in tensorflow\r\n",
    "\r\n",
    "<br/>\r\n",
    "Preprocessed images\r\n",
    "- made black and white\r\n",
    "\r\n",
    "\r\n",
    "<br/>\r\n",
    "Applied transformations to the images to create new training data\r\n",
    "- zoom applied to some\r\n",
    "- random rotation applied to some\r\n",
    "<br/>\r\n",
    "Used SAM Optimizer, this estimates a sharpness-aware gradient \r\n",
    "<br/>\r\n",
    "<br/>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tensorflow network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "I downloaded the dataset from keras datasets. I loaded this into memory because I have hella on my local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-41566bf74ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Show a couple of images to make sure they are in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(train_data[i])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "# from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create one hot matrix of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def one_hotify(np_array):\n",
    "    nb_classes = np_array.max()+1\n",
    "    targets = np.array([np_array]).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]\n",
    "    \n",
    "train_labels = one_hotify(train_labels)\n",
    "test_labels = one_hotify(test_labels)\n",
    "\n",
    "labels_catagories = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Shuffle datasets\n",
    "I don't know if this is strictly nessecary but I figured it wouldn't hurt, other than a small amount of compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_data, train_labels = shuffle(train_data, train_labels)\n",
    "\n",
    "plt.imshow(train_data[0])\n",
    "plt.show\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalize the images\n",
    "\n",
    "Image pixel data has values between 0 and 255 so here we are normalizing between 0 and 1, should help with compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.astype('float32')/255\n",
    "test_data = test_data.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Break into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data, train_valid = train_data[5000:], train_data[:5000]\n",
    "train_labels, valid_labels = train_labels[5000:], train_labels[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below is the data generator for training images. This augments the images before training but in a generator form so\n",
    "the whole augmented dataset isn't in memory, only the batch size being used. Decreased Training time per epoch and \n",
    "allowedtraining data to be augmented for each batch instead of just once like it was before. This increased model \n",
    "accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(width_shift_range=.12, height_shift_range=.12, horizontal_flip=True, zoom_range=.2)\n",
    "\n",
    "datagen_train.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Image Augementation: Pre-ImageDataGenerator\n",
    "\n",
    "Below was a decent Idea to augment the images but using tensorflow.keras imagePreprocessing proved to be faster and \n",
    "I was able to make a generator out of that so I didn't have to put the whole dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create an image manipulator to adjust images zoom and rotation\r\n",
    "\r\n",
    "import PIL\r\n",
    "from PIL import Image\r\n",
    "import random\r\n",
    "\r\n",
    "random.seed(69)\r\n",
    "\r\n",
    "def image_augment(np_image):\r\n",
    "    # create image from array\r\n",
    "    im = Image.fromarray(np_image)\r\n",
    "    og_height, og_width = im.size\r\n",
    "    \r\n",
    "    # get augmentation parameters from random\r\n",
    "    rotation = random.randint(5,34)\r\n",
    "    crop_pix = 4\r\n",
    "    if rotation < 19:\r\n",
    "        crop_pix = 3\r\n",
    "    if rotation < 10:\r\n",
    "        crop_pix = 2\r\n",
    "    crop = (crop_pix, crop_pix, og_height-crop_pix, og_width-crop_pix)\r\n",
    "    x_flip = bool(random.getrandbits(1))\r\n",
    "    y_flip = bool(random.getrandbits(1))\r\n",
    "    \r\n",
    "    # augment image\r\n",
    "    if x_flip and y_flip:\r\n",
    "        return np.array(im.rotate(rotation).crop(crop).transpose(PIL.Image.FLIP_LEFT_RIGHT).transpose(PIL.Image.FLIP_TOP_BOTTOM).resize((32, 32)))\r\n",
    "    elif x_flip:\r\n",
    "        return np.array(im.rotate(rotation).crop(crop).transpose(PIL.Image.FLIP_LEFT_RIGHT).resize((32, 32)))\r\n",
    "    elif y_flip:\r\n",
    "        return np.array(im.rotate(rotation).crop(crop).transpose(PIL.Image.FLIP_TOP_BOTTOM).resize((32, 32)))\r\n",
    "    else: \r\n",
    "        return np.array(im.rotate(rotation).crop(crop).resize((32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This section was the loop to augment the images and save as a new dataset as originally planned, but as discussed above, using\n",
    "a generator proved to be much more time and memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Create augmented versions of all images\r\n",
    "import time\r\n",
    "\r\n",
    "start = time.time()\r\n",
    "\r\n",
    "for i in range(len(train_data)):\r\n",
    "    aug = np.reshape(image_augment(train_data[i]), (1, 32, 32, 3))\r\n",
    "    train_data = np.concatenate((train_data, aug), axis=0)\r\n",
    "    if i % 1000 == 0:\r\n",
    "        int_time = time.time()\r\n",
    "        print(f'Time to modify {i} images: {round(int_time - start)}s')\r\n",
    "\r\n",
    "end = time.time()\r\n",
    "\r\n",
    "train_labels = np.concatenate((train_labels, train_labels), axis=0)\r\n",
    "\r\n",
    "print(f'Time to create augmented data: {round(end - start)}s')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "This section is loading the data pre-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # save or load data because that took hella long to process\r\n",
    "# \r\n",
    "import os.path\r\n",
    "import pickle\r\n",
    "from os import path\r\n",
    "\r\n",
    "path_data = 'Cifar10_train_data'\r\n",
    "path_labels = 'Cifar10_train_labels'\r\n",
    "\r\n",
    "if path.exists(path_data):\r\n",
    "    pickle_in = open(path_data, 'rb')\r\n",
    "    train_data = pickle.load(pickle_in)\r\n",
    "else:\r\n",
    "    pickle_out = open(path_data, 'wb')\r\n",
    "    pickle.dump(train_data, pickle_out)\r\n",
    "    pickle_out.close()\r\n",
    "\r\n",
    "if path.exists(path_labels):\r\n",
    "    pickle_in = open(path_labels, 'rb')\r\n",
    "    train_labels = pickle.load(pickle_in)\r\n",
    "else:\r\n",
    "    pickle_out = open(path_labels, 'wb')\r\n",
    "    pickle.dump(train_labels, pickle_out)\r\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This section just compared the first 9 images before and after modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # plot original first 9 images\r\n",
    "for i in range(9):\r\n",
    "    plt.subplot(330 + 1 + i)\r\n",
    "    plt.imshow(train_data[i])\r\n",
    "plt.show\r\n",
    "\r\n",
    "# plot augmented first 9 images\r\n",
    "for i in range(9):\r\n",
    "    plt.subplot(330 + 1 + i)\r\n",
    "    plt.imshow(image_augment(train_data[i]))\r\n",
    "plt.show\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Network design: ResNet 101 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, Input, ZeroPadding2D, MaxPooling2D, Flatten, Dense, AveragePooling2D, Dropout\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## identity residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def res_id(x, filters):\n",
    "    skip = x\n",
    "    f1, f2 = filters\n",
    "    \n",
    "    reg = .001\n",
    "  \n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(relu)(x)\n",
    "   \n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(relu)(x)\n",
    "\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Add()([x, skip])\n",
    "    x = Activation(relu)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convolution residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def res_conv(x, s, filters):\n",
    "\n",
    "    skip = x\n",
    "    f1, f2 = filters\n",
    "    \n",
    "    reg = .001\n",
    "    \n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(relu)(x)\n",
    "    \n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(relu)(x)\n",
    "    \n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(reg))(skip)\n",
    "    skip = BatchNormalization()(skip)\n",
    "    \n",
    "    x = Add()([x, skip])\n",
    "    x = Activation(relu)(x)\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Main structure of ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def resnet101():\n",
    "\n",
    "    # Part 1\n",
    "    in_image = Input(shape=(train_data.shape[1], train_data.shape[2], train_data.shape[3]))\n",
    "    x = ZeroPadding2D(padding=(3, 3))(in_image)\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(relu)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    \n",
    "    # Part 2\n",
    "    filt = (64, 256)\n",
    "    x = res_conv(x, s=1, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    \n",
    "    # Part 3\n",
    "    filt = (128, 512)\n",
    "    x = res_conv(x, s=2, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    \n",
    "    # Part 4\n",
    "    filt = (256, 1024)\n",
    "    x = res_conv(x, s=2, filters=filt)\n",
    "    for i in range(22):\n",
    "        x = res_id(x, filters=filt)\n",
    "    \n",
    "    # Part 5  \n",
    "    filt = (512, 2048)\n",
    "    x = res_conv(x, s=2, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    x = res_id(x, filters=filt)\n",
    "    \n",
    "    # End\n",
    "    x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(len(train_labels[0]), activation='softmax', kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=in_image, outputs=x, name='ResNet101')\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = resnet101()\r\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 250\n",
    "SAVEPATH = f'weights/ResNet101_bs-{BATCH_SIZE}_ep-{EPOCHS}_{int(time.time())}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tensorboard Callback to plot the loss and accuray while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove old file or create new for the callback to save to\n",
    "log_dir = f'cifar_logs\\\\{BATCH_SIZE}-{EPOCHS}'\n",
    "if not os.path.exists(f'cifar_logs/{BATCH_SIZE}-{EPOCHS}'):\n",
    "    os.mkdir(f'cifar_logs/{BATCH_SIZE}-{EPOCHS}')\n",
    "    \n",
    "if os.path.exists(f'cifar_logs/{BATCH_SIZE}-{EPOCHS}/train'):\n",
    "    shutil.rmtree(f'cifar_logs/{BATCH_SIZE}-{EPOCHS}/train')\n",
    "    \n",
    "if os.path.exists(f'cifar_logs/{BATCH_SIZE}-{EPOCHS}/validation'):\n",
    "    shutil.rmtree(f'cifar_logs/{BATCH_SIZE}-{EPOCHS}/validation')\n",
    "\n",
    "loss_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Model checkpoint callback to save weights while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights_callback = ModelCheckpoint(filepath=SAVEPATH, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "# weights_callback = ModelCheckpoint(filepath=SAVEPATH, save_weights_only=True, save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Learning rate decay callback to adjust learning rate while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lrdecay(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 250:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 210:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "  # if epoch < 40:\n",
    "  #   return 0.01\n",
    "  # else:\n",
    "  #   return 0.01 * np.math.exp(0.03 * (40 - epoch))\n",
    "lrdecay = LearningRateScheduler(lrdecay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tried two different optimizing functions while training this network \n",
    "\n",
    "opt = Adam(learning_rate=.0001)\n",
    "opt = SGD(lr=.001, momentum=.8, decay=.001/100, nesterov=True)\n",
    "\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load weights from the model with the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load weights from best performing network trained from scratch\r\n",
    "model.load_weights('weights/ResNet101_bs-256_ep-160_1604135162.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to actually train\r\n",
    "\r\n",
    "history = model.fit(datagen_train.flow(train_data, train_labels, batch_size=BATCH_SIZE),\r\n",
    "                    steps_per_epoch=train_data.shape[0] // BATCH_SIZE,\r\n",
    "                    epochs=EPOCHS, verbose=2, callbacks=[loss_callback, weights_callback],\r\n",
    "                    validation_data=(train_valid, valid_labels),\r\n",
    "                    validation_steps=train_valid.shape[0] // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plots the training variables of the model fit above\r\n",
    "\r\n",
    "print(history.history)\r\n",
    "\r\n",
    "plt.plot(history.history['val_loss'])\r\n",
    "plt.title('Validation loss history')\r\n",
    "plt.ylabel('Loss value')\r\n",
    "plt.xlabel('No. epoch')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "plt.plot(history.history['loss'])\r\n",
    "plt.title('Loss history')\r\n",
    "plt.ylabel('Loss value')\r\n",
    "plt.xlabel('No. epoch')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "plt.plot(history.history['accuracy'])\r\n",
    "plt.title('Accuracy history')\r\n",
    "plt.ylabel('Loss value')\r\n",
    "plt.xlabel('No. epoch')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "plt.plot(history.history['val_accuracy'])\r\n",
    "plt.title('Validation Accuracy history')\r\n",
    "plt.ylabel('Loss value')\r\n",
    "plt.xlabel('No. epoch')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluate Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the test data on the model\r\n",
    "# this should give 85.8% accuracy\r\n",
    "\r\n",
    "result = model.evaluate(test_data, test_labels)\r\n",
    "\r\n",
    "print(f'Test Accuracy of the model: {result[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove old tensorflow model from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\r\n",
    "\r\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Doing Better\n",
    "\n",
    "This was all well and good for a model but I want to do better, especially with the resnet101 I coded from scratch.\n",
    "To do this I used google's Big Transfer (BiT). This essentially uses a resnet101 trained on ImageNet-21k (a massive \n",
    "dataset with 21k categories, lots of training time on many GPUs was put into these models by google)\n",
    "\n",
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_url = \"https://tfhub.dev/google/bit/m-r101x1/1\"\n",
    "module = hub.KerasLayer(model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create model \r\n",
    "Here we use some transfer learning trickery and add a Dense layer as the head with size of 10 so we are able to predict our 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BiTResNet101(Model):\n",
    "\n",
    "    def __init__(self, num_classes, module):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.head = Dense(num_classes, kernel_initializer='zeros')\n",
    "        self.bit_module = module\n",
    "    \n",
    "    def call(self, images):\n",
    "        bit_embedding = self.bit_module(images)\n",
    "        return self.head(bit_embedding)\n",
    "\n",
    "model = BiTResNet101(num_classes=len(train_labels[0]), module=module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = .1\n",
    "\n",
    "lr_decay = PiecewiseConstantDecay(boundaries=[3000, 6000, 9000], values=[lr, lr*.1, lr*.001, lr*.0001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "opt = SGD(learning_rate=lr_decay, momentum=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=loss_func, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "history = model.fit(datagen_train.flow(train_data, train_labels, batch_size=BATCH_SIZE),\n",
    "                    steps_per_epoch=train_data.shape[0] // BATCH_SIZE,\n",
    "                    epochs=10, \n",
    "                    verbose=2,\n",
    "                    validation_data=(train_valid, valid_labels),\n",
    "                    validation_steps=train_valid.shape[0] // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(f'Test Accuracy of the model: {result[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "device_name = tensorflow.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Original Pytorch Stuff Below\n",
    "I dislike Pytorch so I did everything in tesorflow above. The only change I made to this was commenting the last two\n",
    "cells so I didn't spend any time training that default model.\n",
    "\n",
    "First, import the packages or modules required for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K_4CLE8YCNn_",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCMc2Qo2IF0F"
   },
   "source": [
    "### Loading and normalizing \n",
    "\n",
    "Using torchvision, it’s extremely easy to load CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4X0_KRbcGj6K",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4-1q71QrGj3g",
    "outputId": "768c5249-69af-485d-8220-423ac7d02e15",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = random_split(trainset, [42000,8000])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, \n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "vfY9V9KwLDFF",
    "outputId": "d532742d-610c-475e-c274-adbd3b7bdc2d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8WhH_4KCNo3"
   },
   "source": [
    "## Define the Model\n",
    "\n",
    "(We will cover hybridize next week. It often makes your model run faster, but you can ignore what it means for this homework.)\n",
    "\n",
    "Here, we build the residual blocks based on the HybridBlock class, which is slightly different than the implementation described in the [“Residual networks (ResNet)”](http://d2l.ai/chapter_convolutional-neural-networks/resnet.html) section. This is done to improve execution efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "colab": {},
    "colab_type": "code",
    "id": "I-W_mjuYCNo3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy-sc_10CNo_",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YsKgVvWCNpC"
   },
   "source": [
    "## Define the Training Functions\n",
    "\n",
    "We will select the model and tune hyper-parameters according to the model's performance on the validation set. Next, we define the model training function `train`. We record the training time of each epoch, which helps us compare the time costs of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "JyoNJsjFCNpG",
    "outputId": "e2692bc2-5175-41dd-d0f2-cc0a131379b4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# best_val_acc = -1000\n",
    "# best_val_model = None\n",
    "# for epoch in range(10):  \n",
    "#     net.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_acc = 0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.cuda(),labels.cuda()\n",
    "# \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = net(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# \n",
    "#         # print statistics\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         out = torch.argmax(outputs.detach(),dim=1)\n",
    "#         assert out.shape==labels.shape\n",
    "#         running_acc += (labels==out).sum().item()\n",
    "#     print(f\"Train loss {epoch+1}: {running_loss/len(trainset)},Train Acc:{running_acc*100/len(trainset)}%\")\n",
    "#     \n",
    "#     correct = 0\n",
    "#     net.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for inputs,labels in valloader:\n",
    "#             out = net(inputs.cuda()).cpu()\n",
    "#             out = torch.argmax(out,dim=1)\n",
    "#             acc = (out==labels).sum().item()\n",
    "#             correct += acc\n",
    "#     print(f\"Val accuracy:{correct*100/len(valset)}%\")\n",
    "#     if correct>best_val_acc:\n",
    "#         best_val_acc = correct\n",
    "#         best_val_model = deepcopy(net.state_dict())\n",
    "#     lr_scheduler.step()\n",
    "#     \n",
    "# print('Finished Training')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "v6fzf0qNOqV9",
    "outputId": "c76a009c-838c-4b49-8888-220299ae8fdc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# correct = 0\n",
    "# net.load_state_dict(best_val_model)\n",
    "# net.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs,labels in testloader:\n",
    "#         out = net(inputs.cuda()).cpu()\n",
    "#         out = torch.argmax(out,dim=1)\n",
    "#         acc = (out==labels).sum().item()\n",
    "#         \n",
    "#         correct += acc\n",
    "# print(f\"Test accuracy: {correct*100/len(testset)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaTLx0URXaBq"
   },
   "source": [
    "## Hints to Improve Your Results\n",
    "\n",
    "* You'd better use a GPU machine to run it, otherwise it'll be quite slow.\n",
    "* Revise the simple CNN model\n",
    "* Revise the *transforms* function by using some image augumentation techniques\n",
    "* Tune hyper-parameters, such as batch_size\n",
    "* Change to another network, such as ResNet-34 or Inception\n",
    "* Using the pre-trained models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (Cifar)",
   "language": "python",
   "name": "cifar"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}